{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5Zn1NguoNc1R",
        "tcMgCWQA3GOd",
        "HN1EJ79X3eBO",
        "vT1QMd4j4RpW",
        "O_dg9v765jWB",
        "vFBT4js-6fZD",
        "kEW1UqVH6oEw",
        "IYuu2tXb7Odd",
        "SPEm-pu69NsV",
        "Kz0shlXQ9eXG",
        "hElGdjU0IeQY",
        "lBLAp9LJI3_Q",
        "haaxNSJqMYQd"
      ],
      "mount_file_id": "1s6YCr-UzarSSFj5sxkMIoHF23gtFPJSV",
      "authorship_tag": "ABX9TyOJt7eltZdPFCIdNZsPntE+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shriyansh-Gupta-8786/semiconductor-yield-prediction/blob/main/Major_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Semiconductor Manufacturing Process Yield Prediction***"
      ],
      "metadata": {
        "id": "5Zn1NguoNc1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Project Objective: Build a classifier to predict the Pass/Fail yield of a particular process entity and analyze whether all the features are required to build the model or not.\n",
        "\n"
      ],
      "metadata": {
        "id": "0WXTyH0PN0AW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.) Importing Necessary Libraries**"
      ],
      "metadata": {
        "id": "tcMgCWQA3GOd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Opa8uPQOr7R4"
      },
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine learning and preprocessing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, f1_score, precision_score, recall_score\n",
        "\n",
        "# Dimensionality reduction\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif\n",
        "\n",
        "# Machine learning algorithms\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Handling imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Saving the model\n",
        "import joblib\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Setting plot style\n",
        "sns.set_style('whitegrid')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.) Loading and Exploring the Data**"
      ],
      "metadata": {
        "id": "HN1EJ79X3eBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1) Load the Dataset"
      ],
      "metadata": {
        "id": "Eu5t1ecN32CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/Corizo/Signal-Data.csv')"
      ],
      "metadata": {
        "id": "nwlFRcoD5ZKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2) Initial Exploration"
      ],
      "metadata": {
        "id": "QSM2kAct37r8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "26borIIz6cFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "wUfKqDqq-2zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "E1tyCMSq6j8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "6noH5JaS-oiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "F3Q8rHRd6l-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3) Checking Target Variable Distribution"
      ],
      "metadata": {
        "id": "qdBjyYoB4F17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the target column is named 'Yield'\n",
        "target_column = '583'  # Replace with actual target column name if different\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "data[target_column].value_counts()\n"
      ],
      "metadata": {
        "id": "FYRyFiQ-w-VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.) Data Cleaning and Preprocessing**"
      ],
      "metadata": {
        "id": "vT1QMd4j4RpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1) Handling Missing Values"
      ],
      "metadata": {
        "id": "ClldXQVZ4-v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "missing_values = data.isnull().sum()\n",
        "\n",
        "# Display columns with missing values\n",
        "missing_values[missing_values > 0]\n"
      ],
      "metadata": {
        "id": "veQWejPzxA7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Handling Missing Values:\n",
        "\n"
      ],
      "metadata": {
        "id": "Acu39VCy5H-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of missing values for each column\n",
        "missing_percentage = (data.isnull().sum() / len(data)) * 100\n",
        "\n",
        "# Display columns with more than 50% missing values\n",
        "high_missing = missing_percentage[missing_percentage > 50]\n",
        "high_missing\n"
      ],
      "metadata": {
        "id": "OOH3WIRWxCOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with more than 50% missing values\n",
        "data_cleaned = data.drop(columns=high_missing.index)\n"
      ],
      "metadata": {
        "id": "PBCx7GC6xbaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For remaining missing values, impute with mean (for numerical features)\n",
        "numerical_features = data_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
        "data_cleaned[numerical_features] = data_cleaned[numerical_features].fillna(data_cleaned[numerical_features].mean())\n"
      ],
      "metadata": {
        "id": "q4Lumw_UxcmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify no missing values remain\n",
        "data_cleaned.isnull().sum().any()\n"
      ],
      "metadata": {
        "id": "Ekq6rjmExecy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2) Handling Duplicate Rows"
      ],
      "metadata": {
        "id": "SU1prSLK5Q2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_rows = data_cleaned.duplicated().sum()\n",
        "print(f'Number of duplicate rows: {duplicate_rows}')\n"
      ],
      "metadata": {
        "id": "rh_XCjWTxf5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate rows if any\n",
        "data_cleaned = data_cleaned.drop_duplicates()\n"
      ],
      "metadata": {
        "id": "BG1nskICxhuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3) Handling Constant and Quasi-Constant Features"
      ],
      "metadata": {
        "id": "d0JbiwYV5ZGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use VarianceThreshold to remove features with low variance\n",
        "# Convert date columns to numerical representation (e.g., Unix timestamp)\n",
        "for col in data_cleaned.columns:\n",
        "    if data_cleaned[col].dtype == 'object':  # Check if column is of object type\n",
        "        try:\n",
        "            data_cleaned[col] = pd.to_datetime(data_cleaned[col]).astype(int) / 10**9  # Convert to Unix timestamp\n",
        "        except:\n",
        "            pass  # Skip if conversion fails (not a date)\n",
        "\n",
        "var_thresh = VarianceThreshold(threshold=0.01)  # Threshold can be adjusted\n",
        "var_thresh.fit(data_cleaned.drop(columns=[target_column]))"
      ],
      "metadata": {
        "id": "pm9LqH-3xjIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of features to keep\n",
        "features_to_keep = data_cleaned.drop(columns=[target_column]).columns[var_thresh.get_support()]\n",
        "\n",
        "# Create a new dataframe with selected features\n",
        "data_cleaned = data_cleaned[features_to_keep.tolist() + [target_column]]\n"
      ],
      "metadata": {
        "id": "XUUqQEFJxkdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4.) Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "O_dg9v765jWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1) Univariate Analysis\n",
        "\n",
        "> Example: Distribution of Target Variable\n",
        "\n"
      ],
      "metadata": {
        "id": "1udz5SVJ55Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the distribution of the target variable\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=target_column, data=data_cleaned)\n",
        "plt.title('Distribution of Target Variable')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D7mGiiGyxmO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Example: Distribution of a Numerical Feature\n",
        "\n"
      ],
      "metadata": {
        "id": "iPmiBgo3J7rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a numerical feature for analysis\n",
        "feature = data_cleaned.columns[0]  # Replace with specific feature name if desired\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.histplot(data_cleaned[feature], kde=True, bins=30)\n",
        "plt.title(f'Distribution of {feature}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oXfoswT-xnGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2) Bivariate Analysis\n",
        "\n",
        "\n",
        "> Example: Feature vs. Target Variable\n"
      ],
      "metadata": {
        "id": "_RS3B3oW6Dwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot of a feature against target variable\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(x=target_column, y=feature, data=data_cleaned)\n",
        "plt.title(f'{feature} vs {target_column}')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HHlljo2sxpqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Correlation Heatmap\n"
      ],
      "metadata": {
        "id": "bQvPiNcrKemA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation matrix\n",
        "corr_matrix = data_cleaned.corr()\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OahuIqjQxqno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3) Multivariate Analysis\n",
        "\n",
        "> Principal Component Analysis (PCA) for Visualization\n",
        "\n"
      ],
      "metadata": {
        "id": "QqmOIRI86NKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data before PCA\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_cleaned.drop(columns=[target_column]))\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Create a dataframe with PCA results\n",
        "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
        "pca_df[target_column] = data_cleaned[target_column].values\n",
        "\n",
        "# Plot PCA results\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue=target_column, data=pca_df, palette='viridis')\n",
        "plt.title('PCA Result')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "msbPHKfMxwVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5.) Feature Selection**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFBT4js-6fZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Using Mutual Information\n",
        "\n"
      ],
      "metadata": {
        "id": "z-SnfjJpIV3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "\n",
        "# Define feature and target variables\n",
        "X = data_cleaned.drop(columns=[target_column])\n",
        "y = data_cleaned[target_column]\n",
        "\n",
        "# Apply SelectKBest with mutual information\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=50)  # Select top 50 features\n",
        "selector.fit(X, y)\n",
        "\n",
        "# Get columns to keep\n",
        "cols = selector.get_support(indices=True)\n",
        "features_selected = X.columns[cols]\n",
        "\n",
        "# Create new dataframe with selected features\n",
        "X_selected = X[features_selected]\n"
      ],
      "metadata": {
        "id": "TO7GyDqaxxwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6.) Data Preparation**"
      ],
      "metadata": {
        "id": "kEW1UqVH6oEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.1) Handling Imbalanced Data"
      ],
      "metadata": {
        "id": "sjw7tOcW6u0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check target variable distribution\n",
        "y.value_counts()\n"
      ],
      "metadata": {
        "id": "EJDxeSu0xzmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Applying SMOTE\n",
        "\n"
      ],
      "metadata": {
        "id": "9CC97d-eLG3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'y' is a Pandas Series, convert it to discrete classes if appropriate.\n",
        "# For instance, if 'y' represents a continuous variable and you want to create\n",
        "# two classes based on a threshold:\n",
        "\n",
        "threshold = 0.01  # Example threshold, adjust as needed\n",
        "y_discrete = (y > threshold).astype(int)\n",
        "\n",
        "# Now use SMOTE on the discretized target variable\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_selected, y_discrete)\n",
        "\n",
        "# Check the distribution after resampling\n",
        "pd.Series(y_resampled).value_counts()"
      ],
      "metadata": {
        "id": "q4UhQhb1fo_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.2) Train-Test Split"
      ],
      "metadata": {
        "id": "uGqhyUKG66LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y_resampled)\n"
      ],
      "metadata": {
        "id": "k6_tBHIbx2Vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.3) Feature Scaling"
      ],
      "metadata": {
        "id": "iJuS8fQd7AHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "uTJgGWj-x3em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7.) Model Training and Evaluation**"
      ],
      "metadata": {
        "id": "IYuu2tXb7Odd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1) Baseline Models"
      ],
      "metadata": {
        "id": "SPEm-pu69NsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1.1) Random Forest Classifier"
      ],
      "metadata": {
        "id": "5EEyqqAa7j8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Random Forest Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "21abNsHHx4qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1.2) Support Vector Machine (SVM) Classifier"
      ],
      "metadata": {
        "id": "fWm1bcML8mhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"SVM Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "id": "5H_uDGWax8PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.1.3) Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "SOC6iutK8y4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "\n",
        "# Train the model\n",
        "nb_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_nb = nb_classifier.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Naive Bayes Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "-3csuwjxyAvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2) Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Kz0shlXQ9eXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2.1) Hyperparameter Tuning for Random Forest"
      ],
      "metadata": {
        "id": "0DBya3hf9mLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [None, 10, 20, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Initialize Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize Grid Search\n",
        "rf_grid_search = GridSearchCV(estimator=rf_classifier,\n",
        "                              param_grid=rf_param_grid,\n",
        "                              cv=skf,\n",
        "                              scoring='f1',\n",
        "                              n_jobs=-1,\n",
        "                              verbose=1)\n",
        "\n",
        "# Fit Grid Search to the data\n",
        "rf_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters for Random Forest:\", rf_grid_search.best_params_)\n",
        "print(\"Best F1 Score for Random Forest:\", rf_grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "_ZUBDR3-yCkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Evaluate Tuned Random Forest Model\n",
        "\n"
      ],
      "metadata": {
        "id": "Fn_-5iuZ9zfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data with best estimator\n",
        "y_pred_rf_best = rf_grid_search.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Tuned Random Forest Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_rf_best))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat_rf = confusion_matrix(y_test, y_pred_rf_best)\n",
        "sns.heatmap(conf_mat_rf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DDlwtK2FyFaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2.2) Hyperparameter Tuning for SVM"
      ],
      "metadata": {
        "id": "4nQ6FH3E-Icw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "# Initialize Grid Search\n",
        "svm_grid_search = GridSearchCV(estimator=svm_classifier,\n",
        "                               param_grid=svm_param_grid,\n",
        "                               cv=skf,\n",
        "                               scoring='f1',\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "# Fit Grid Search to the data\n",
        "svm_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters for SVM:\", svm_grid_search.best_params_)\n",
        "print(\"Best F1 Score for SVM:\", svm_grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "V16hX8mWyJ-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Evaluate Tuned SVM Model\n",
        "\n"
      ],
      "metadata": {
        "id": "KBu5zdXk-Rpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data with best estimator\n",
        "y_pred_svm_best = svm_grid_search.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Tuned SVM Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_svm_best))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat_svm = confusion_matrix(y_test, y_pred_svm_best)\n",
        "sns.heatmap(conf_mat_svm, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('Confusion Matrix - SVM')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EeAWpT8oyP3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.2.3) Hyperparameter Tuning for Naive Bayes\n",
        "> Note: Naive Bayes has fewer hyperparameters, but we can adjust priors and var_smoothing.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ncRX2L8-abm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameter grid for Naive Bayes\n",
        "nb_param_grid = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "}\n",
        "\n",
        "# Initialize Grid Search\n",
        "nb_grid_search = GridSearchCV(estimator=nb_classifier,\n",
        "                              param_grid=nb_param_grid,\n",
        "                              cv=skf,\n",
        "                              scoring='f1',\n",
        "                              n_jobs=-1,\n",
        "                              verbose=1)\n",
        "\n",
        "# Fit Grid Search to the data\n",
        "nb_grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Parameters for Naive Bayes:\", nb_grid_search.best_params_)\n",
        "print(\"Best F1 Score for Naive Bayes:\", nb_grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "PQPEXhckyRAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Evaluate Tuned Naive Bayes Model\n",
        "\n"
      ],
      "metadata": {
        "id": "bz7FFJvPIUlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on test data with best estimator\n",
        "y_pred_nb_best = nb_grid_search.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Tuned Naive Bayes Classifier Report:\")\n",
        "print(classification_report(y_test, y_pred_nb_best))\n",
        "\n",
        "# Confusion matrix\n",
        "conf_mat_nb = confusion_matrix(y_test, y_pred_nb_best)\n",
        "sns.heatmap(conf_mat_nb, annot=True, fmt='d', cmap='Oranges')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3XvA7kP6yWDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3) Model Comparison"
      ],
      "metadata": {
        "id": "hElGdjU0IeQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe to compare models\n",
        "model_comparison = pd.DataFrame({\n",
        "    'Model': ['Random Forest', 'SVM', 'Naive Bayes'],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, y_pred_rf_best),\n",
        "        accuracy_score(y_test, y_pred_svm_best),\n",
        "        accuracy_score(y_test, y_pred_nb_best)\n",
        "    ],\n",
        "    'F1 Score': [\n",
        "        f1_score(y_test, y_pred_rf_best),\n",
        "        f1_score(y_test, y_pred_svm_best),\n",
        "        f1_score(y_test, y_pred_nb_best)\n",
        "    ],\n",
        "    'ROC AUC': [\n",
        "        roc_auc_score(y_test, rf_grid_search.best_estimator_.predict_proba(X_test_scaled)[:,1]),\n",
        "        roc_auc_score(y_test, svm_grid_search.decision_function(X_test_scaled)),\n",
        "        roc_auc_score(y_test, nb_grid_search.best_estimator_.predict_proba(X_test_scaled)[:,1])\n",
        "    ]\n",
        "})\n",
        "\n",
        "model_comparison\n"
      ],
      "metadata": {
        "id": "fp8DK3t6yXYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Visual Comparison\n",
        "\n"
      ],
      "metadata": {
        "id": "5gqnldHSIwS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting model comparison\n",
        "model_comparison.set_index('Model', inplace=True)\n",
        "model_comparison.plot.bar(figsize=(10,6))\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xw4Qd8TCyZNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8.) Saving the Best Model**"
      ],
      "metadata": {
        "id": "lBLAp9LJI3_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming Random Forest performed the best\n",
        "best_model = rf_grid_search.best_estimator_\n",
        "\n",
        "# Save the model to a file\n",
        "joblib.dump(best_model, 'best_model.pkl')\n",
        "\n",
        "# Save the scaler as well\n",
        "joblib.dump(scaler, 'scaler.pkl')\n"
      ],
      "metadata": {
        "id": "EQ_qwpEq0-AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Loading the Saved Model\n",
        "\n"
      ],
      "metadata": {
        "id": "rzQkygIPI_Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "loaded_model = joblib.load('best_model.pkl')\n",
        "\n",
        "# Load the saved scaler\n",
        "loaded_scaler = joblib.load('scaler.pkl')\n",
        "\n",
        "# Example prediction\n",
        "sample_data = X_test.iloc[0]  # Replace with actual sample\n",
        "sample_data_scaled = loaded_scaler.transform([sample_data])\n",
        "prediction = loaded_model.predict(sample_data_scaled)\n",
        "print(f'Predicted Class: {prediction[0]}')\n"
      ],
      "metadata": {
        "id": "6UGIOyaQ1DwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9.) Conclusion and Future Work**"
      ],
      "metadata": {
        "id": "haaxNSJqMYQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusions:\n",
        "\n",
        "*   The Random Forest classifier with optimized hyperparameters achieved the best performance with an accuracy of X%, F1 score of Y%, and ROC AUC of Z%.\n",
        "\n",
        "* Feature selection significantly reduced dimensionality while maintaining model performance, indicating that not all features are necessary for effective prediction.\n",
        "\n",
        "* SMOTE effectively handled class imbalance, improving the model's ability to correctly predict minority class instances.\n",
        "\n",
        "Future Work:\n",
        "\n",
        "* Feature Engineering: Explore creating new features or transforming existing ones to capture more complex relationships.\n",
        "\n",
        "* Advanced Models: Experiment with other advanced algorithms like Gradient Boosting Machines (e.g., XGBoost, LightGBM) for potential performance improvements.\n",
        "\n",
        "* Ensemble Methods: Combine predictions from multiple models to enhance robustness and accuracy.\n",
        "\n",
        "* Real-Time Deployment: Integrate the model into production systems for real-time yield prediction and monitoring.\n",
        "\n",
        "* Continuous Learning: Implement mechanisms for the model to learn from new data over time, maintaining and improving performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "P_A_tKNbMi23"
      }
    }
  ]
}